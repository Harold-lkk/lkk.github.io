<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Blog on Qwen</title>
    <link>http://qwenlm.github.io/zh/blog/</link>
    <description>Recent content in Blog on Qwen</description>
    <image>
      <url>http://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>http://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 11 May 2024 18:10:00 +0800</lastBuildDate><atom:link href="http://qwenlm.github.io/zh/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Qwen-Max-0428模型介绍</title>
      <link>http://qwenlm.github.io/zh/blog/qwen-max-0428/</link>
      <pubDate>Sat, 11 May 2024 18:10:00 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/zh/blog/qwen-max-0428/</guid>
      <description>API DEMO DISCORD
此前，我们开源了Qwen1.5系列的模型，参数规模最小至5亿，最大至1100亿。这一次，我们推出更大规模模型Qwen-Max-0428（通义千问网页端及APP产品版本从2.1升级至2.5）。Qwen-Max-0428是经过指令微调的Chat模型。近期该模型登陆了Chatbot Arena，并登榜前十。此外，我们在MT-Bench的评测上也观察到该模型的表现显著优于Qwen1.5-110B-Chat。
Models MT-Bench Arena Qwen1.5-110B-Chat 8.88 1172 Qwen-Max-0428 8.96 1186 我们也在Hugging Face上提供了Demo服务（链接）：
同时我们也提供了DashScope API服务（链接）。目前API服务已经支持OpenAI API格式，示例如下所示：
from openai import OpenAI client = OpenAI( api_key=&amp;#34;$your-dashscope-api-key&amp;#34;, base_url=&amp;#34;https://dashscope.aliyuncs.com/compatible-mode/v1&amp;#34; ) completion = client.chat.completions.create( model=&amp;#34;qwen-max&amp;#34;, messages=[{&amp;#39;role&amp;#39;: &amp;#39;system&amp;#39;, &amp;#39;content&amp;#39;: &amp;#39;You are a helpful assistant.&amp;#39;}, {&amp;#39;role&amp;#39;: &amp;#39;user&amp;#39;, &amp;#39;content&amp;#39;: &amp;#39;Tell me something about large language models.&amp;#39;}] ) print(completion.choices[0].message) 此外，Qwen-Max-0428已上线通义千问网页端及APP。欢迎体验！
引用 @misc{qwen1.5, title = {Introducing Qwen1.5}, url = {https://qwenlm.github.io/blog/qwen1.5/}, author = {Qwen Team}, month = {February}, year = {2024} } </description>
    </item>
    
    <item>
      <title>Qwen1.5-110B：Qwen1.5系列的首个千亿参数开源模型</title>
      <link>http://qwenlm.github.io/zh/blog/qwen1.5-110b/</link>
      <pubDate>Thu, 25 Apr 2024 13:33:00 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/zh/blog/qwen1.5-110b/</guid>
      <description>GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
简介 近期开源社区陆续出现了千亿参数规模以上的大模型，这些模型都在各项评测中取得杰出的成绩。今天，我们开源1100亿参数的Qwen1.5系列首个千亿参数模型Qwen1.5-110B，该模型在基础能力评估中与Meta-Llama3-70B相媲美，在Chat评估中表现出色，包括MT-Bench和AlpacaEval 2.0。
模型特性 Qwen1.5-110B与其他Qwen1.5模型相似，采用了相同的Transformer解码器架构。它包含了分组查询注意力（GQA），在模型推理时更加高效。该模型支持32K tokens的上下文长度，同时它仍然是多语言的，支持英、中、法、西、德、俄、日、韩、越、阿等多种语言。
模型效果 我们对基础语言模型进行了一系列评估，并与最近的SOTA语言模型Meta-Llama3-70B以及Mixtral-8x22B进行了比较。
Qwen1.5-110B Qwen1.5-72B Llama-3-70B Mixtral-8x22B MMLU 80.4 77.5 79.5 77.8 TheoremQA 34.9 29.3 32.0 35.9 GPQA 35.9 36.3 36.4 34.3 Hellaswag 87.5 86.0 88.0 88.7 BBH 74.8 65.5 76.6 69.2 ARC-C 69.6 65.9 68.8 70.7 GSM8K 85.4 79.5 79.2 78.6 MATH 49.6 34.1 41.0 41.7 HumanEval 52.4 41.5 45.7 45.1 MBPP 58.1 53.4 55.1 71.2 上述结果显示，新的110B模型在基础能力方面至少与Llama-3-70B模型相媲美。在这个模型中，我们没有对预训练的方法进行大幅改变，因此我们认为与72B相比的性能提升主要来自于增加模型规模。
我们还在MT-Bench和AlpacaEval 2.</description>
    </item>
    
    <item>
      <title>与 CodeQwen1.5 结对编程</title>
      <link>http://qwenlm.github.io/zh/blog/codeqwen1.5/</link>
      <pubDate>Tue, 16 Apr 2024 13:33:00 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/zh/blog/codeqwen1.5/</guid>
      <description>GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
简介 代码助手，是一种基于 LLMs 的智能化的编程工具，它可以帮助程序员更高效、更准确的编写代码，使得整个软件开发过程更加流畅和高效。然而流行的代码助手，比如 Github Copilot，依赖于闭源的商业模型，不仅昂贵还会引起如隐私、安全、版权等方面的担忧。幸运的是，开源社区正在致力于打造开放代码模型来实现开放的代码助手。近期涌现出了一批优秀的 Open CodeLLMs，比如 StarCoder2、CodeLlama、DeepSeek-Coder 等，提供了一条新的路径，但仍然值得探索。
今天，我们非常激动地和大家介绍来自 Qwen1.5 开源家族的新成员，一个代码专家模型 CodeQwen1.5! CodeQwen1.5 基于 Qwen 语言模型初始化，拥有 7B 参数的模型，其拥有 GQA 架构，经过了 ~3T tokens 代码相关的数据进行预训练，共计支持 92 种编程语言、且最长支持 64K 的上下文输入。效果方面，CodeQwen1.5 展现出了非凡的代码生成、长序列建模、代码修改、SQL 能力等,该模型可以大大提高开发人员的工作效率，并在不同的技术环境中简化软件开发工作流程。
CodeQwen 是基础的 Coder 代码生成是大语言模型的关键能力之一，期待模型将自然语言指令转换为具有精确的、可执行的代码。仅拥有 70 亿参数的 CodeQwen1.5 在基础代码生成能力上已经超过了更尺寸的模型，进一步缩小了开源代码 LLM 和 GPT-4 之间的编码能力差距。我们对 HumanEval 和 MBPP 进行了评估，下面是具体的比较。
Model Size HumanEval 0-shot HumanEval+ 0-shot MBPP 0-shot MBPP+ 0-shot MBPP 3-shot Base Model CodeLlama-Base 7B 33.5 25.</description>
    </item>
    
    <item>
      <title>Qwen1.5-32B：Qwen1.5语言模型系列的最后一块拼图</title>
      <link>http://qwenlm.github.io/zh/blog/qwen1.5-32b/</link>
      <pubDate>Tue, 02 Apr 2024 13:33:00 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/zh/blog/qwen1.5-32b/</guid>
      <description>GITHUB HUGGING FACE MODELSCOPE DEMO WeChat
简介 开源社区长期以来一直在寻求一种能在性能、效率和内存占用之间达到理想平衡的模型。尽管出现了诸如Qwen1.5-72B和DBRX这样的SOTA模型，但这些模型持续面临诸如内存消耗巨大、推理速度缓慢以及显著的微调成本等问题。当前，参数量约30B的模型往往在这方面被看好，得到很多用户的青睐。顺应这一趋势，我们推出Qwen1.5语言模型系列的最新成员：Qwen1.5-32B和Qwen1.5-32B-Chat。
过去数月中，我们精心研发了Qwen1.5-32B基础模型，旨在对标甚至超越当前最先进的30B模型所设定的性能基准。同时，我们在对齐方面取得了进展，特别是在RLHF方面，以提升Qwen1.5-32B-Chat的对话能力。
模型效果 Qwen1.5-32B 是 Qwen1.5 语言模型系列的最新成员，除了模型大小外，其在模型架构上除了GQA几乎无其他差异。GQA能让该模型在模型服务时具有更高的推理效率潜力。
以下我们将对比展示其与参数量约为30B或更大的当前最优（SOTA）模型在基础能力评估、chat评估以及多语言评估方面的性能。以下是对于基础语言模型能力的评估结果：
Model MMLU C-Eval GSM8K MATH HumanEval MBPP BBH CMMLU Llama2-34B 62.6 - 42.2 6.2 22.6 33.0 44.1 - Yi-34B 76.3 81.4 67.2 14.4 23.2 41.0 54.3 83.7 Mixtral-8x7B 70.6 - 74.4 28.4 40.2 60.7 - - Qwen1.5-72B 77.5 84.1 79.5 34.1 41.5 53.4 65.5 83.5 Qwen1.5-32B 73.4 83.5 77.4 36.1 37.2 49.4 66.8 82.3 我们的32B模型在多种任务上展现出颇具竞争力的表现，涵盖MMLU、GSM8K、HumanEval以及BBH等。相较于72B参数模型，Qwen1.5-32B虽在性能上有轻微下降，但在多数任务中仍优于其他30B级别模型，如Llama2-34B和Mixtral-8x7B。</description>
    </item>
    
    <item>
      <title>Qwen1.5-MoE: 1/3的激活参数量达到7B模型的性能</title>
      <link>http://qwenlm.github.io/zh/blog/qwen-moe/</link>
      <pubDate>Thu, 28 Mar 2024 11:31:44 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/zh/blog/qwen-moe/</guid>
      <description>GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
介绍 今天，我们推出Qwen系列的首个MoE模型，Qwen1.5-MoE-A2.7B。它仅拥有27亿个激活参数，但其性能却能与当前最先进的70亿参数模型，如Mistral 7B和Qwen1.5-7B相媲美。相较于包含65亿个Non-Embedding参数的Qwen1.5-7B，Qwen1.5-MoE-A2.7B只有20亿个Non-Embedding参数，约为原模型大小的三分之一。此外，相比Qwen1.5-7B，Qwen1.5-MoE-A2.7B的训练成本降低了75%，推理速度则提升至1.74倍。
模型结构 我们在Qwen1.5-MoE模型中采用了特别设计的MoE架构。通常情况下，如Mixtral等方法所示，每个transformer block中的MoE层会配备8个expert，并采用top-2门控策略进行routing。这种配置还存在很大的优化空间。我们对这一架构进行了多项改进：
Finegrained experts 初始化 新的routing机制 DeepSeek-MoE和DBRX已经证明了finegrained experts的有效性。从FFN层过渡到MoE层时，我们一般只是简单地复制多次FFN来实现多个expert。而finegrained experts的目标是在不增加参数数量的前提下生成更多expert。为了实现这一点，我们将单个FFN分割成几个部分，每个部分作为一个独立的expert。我们设计了具有总共64个expert的的MoE，对比其他配置，我们认为这个实现能达到效果和效率的最优。
模型初始化阶段至关重要。初步实验表明，从零开始训练MoE模型可能效率低下，且难以提升至预期的最优性能水平。因此，我们首先利用已有的Qwen-1.8B，将其改造为Qwen1.5-MoE-A2.7B。此外，在初始化阶段引入随机性可以显著加快收敛速度，并在整个预训练过程中带来更好的整体性能表现。
目前，一个明显的趋势是在MoE中实现共享expert与routing expert。从更宏观的角度看，这是一种广义的routing方法，因为在没有共享expert的情况下，实际上就退化为传统的MoE路由设置。对于Qwen1.5-MoE-A2.7B模型，我们在其中整合了4个总是被激活的共享expert和每次只激活其中4个的60个routing expert。这种方式非常灵活，同时在我们实验中效率最佳。
性能 为了全面评估和展示Qwen1.5-MoE-A2.7B的能力和优势，我们对base模型和chat模型进行了评估。对于base模型，我们在MMLU、GSM8K和HumanEval评估了其语言理解、数学和代码能力。此外，为了评估其多语言能力，我们按照Qwen1.5的评测方法在数学、理解、考试和翻译等多个领域的多语言基准测试中进行了测试，并在&amp;quot;Multilingual&amp;quot;列中给出了综合得分。对于chat模型，我们没有使用传统的基准测试，而是使用MT-Bench进行了测试。
在这个比较分析中，我们将Qwen1.5-MoE-A2.7B与最好的7B模型，比如Mistral-7B（base模型为v0.1，chat模型为v0.2）、Gemma-7B以及Qwen1.5-7B进行了对比。此外，我们还将其与具有相似参数数量的MoE模型DeepSeekMoE 16B进行了比较。结果如下表所示：
Model MMLU GSM8K HumanEval Multilingual MT-Bench Mistral-7B 64.1 47.5 27.4 40.0 7.60 Gemma-7B 64.6 50.9 32.3 - - Qwen1.5-7B 61.0 62.5 36.0 45.2 7.60 DeepSeekMoE 16B 45.0 18.8 26.8 - 6.93 Qwen1.5-MoE-A2.7B 62.5 61.5 34.2 40.8 7.17 Qwen1.5-MoE-A2.7B在与最佳的7B模型相比取得了非常接近的性能。然而，我们发现在chat模型方面仍有改进的空间。我们将继续研究如何更加有效地微调MoE模型。
训练成本与推理效率 MoE模型的训练成本与dense模型存在显著差异。尽管MoE模型通常拥有更多的参数，但由于其稀疏性，训练开销可以显著降低。我们先对比各个模型的三个关键参数，分别是总参数数量、激活参数数量和Non-embedding参数：
Model #Parameters #(Activated) Parameters #(Activated) Non-embedding parameters Mistral-7B 7.</description>
    </item>
    
    <item>
      <title>Qwen1.5 介绍</title>
      <link>http://qwenlm.github.io/zh/blog/qwen1.5/</link>
      <pubDate>Sun, 04 Feb 2024 13:33:00 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/zh/blog/qwen1.5/</guid>
      <description>GITHUB HUGGING FACE MODELSCOPE DEMO WeChat
简介 最近几个月，我们专注探索如何构建一个真正「卓越」的模型，并在此过程中不断提升开发者的使用体验。农历新年到来之际，我们推出通义千问开源模型1.5版本: Qwen1.5。我们开源了包括0.5B、1.8B、4B、7B、14B、32B、72B和110B共计8个不同规模的Base和Chat模型，, 以及一个MoE模型（点击博客 了解详情），并同步放出了各尺寸模型对应的量化模型。
此次更新中，我们不仅像之前一样提供Int4和Int8的GPTQ模型，还提供了AWQ以及GGUF量化模型。为了提升开发者体验，我们将Qwen1.5的代码正式合并到HuggingFace transformers代码库中，所以现在可以直接使用 transformers&amp;gt;=4.37.0 原生代码，而无需指定 trust_remote_code 选项即可进行开发。
我们已经与vLLM、SGLang（用于部署）、AutoAWQ、AutoGPTQ（用于量化）、Axolotl、LLaMA-Factory（用于微调）以及llama.cpp（用于本地 LLM 推理）等框架合作，所有这些框架现在都支持 Qwen1.5。Qwen1.5 系列可在 Ollama 和 LMStudio 等平台上使用。此外，API 服务不仅在 DashScope 上提供，还在 together.ai 上提供，全球都可访问。请访问here开始使用，我们建议您试用Qwen1.5-72B-chat。
相较于以往版本，本次更新我们着重提升Chat模型与人类偏好的对齐程度，并且显著增强了模型的多语言处理能力。在序列长度方面，所有规模模型均已实现 32768 个 tokens 的上下文长度范围支持。同时，预训练 Base 模型的质量也有关键优化，有望在微调过程中为您带来更佳体验。这次迭代是我们朝向「卓越」模型目标所迈进一个坚实的步伐。
模型效果 为了全面洞悉 Qwen1.5 的效果表现，我们对 Base 和 Chat 模型在一系列基础及扩展能力上进行了详尽评估，包括如语言理解、代码、推理等在内的基础能力，多语言能力，人类偏好对齐能力，智能体能力，检索增强生成能力（RAG）等。
基础能力 关于模型基础能力的评测，我们在 MMLU（5-shot）、C-Eval、Humaneval、GS8K、BBH 等基准数据集上对 Qwen1.5 进行了评估。
Model MMLU C-Eval GSM8K MATH HumanEval MBPP BBH CMMLU GPT-4 86.4 69.9 92.0 45.8 67.0 61.8 86.7 71.0 Llama2-7B 46.</description>
    </item>
    
    <item>
      <title>Qwen-VL全新升级！</title>
      <link>http://qwenlm.github.io/zh/blog/qwen-vl/</link>
      <pubDate>Thu, 25 Jan 2024 13:33:00 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/zh/blog/qwen-vl/</guid>
      <description>我们在Qwen语言模型的基础上，结合此前我们提出的多模态多任务训练，以解决多模态模型在泛化能力上的局限性，并于2023年9月开源了多模态模型Qwen-VL。最近，Qwen-VL系列有了重大升级，推出了两个增强版本：Qwen-VL-Plus和Qwen-VL-Max。这两个版本的关键提升包括：
显著提升与图像相关的推理能力； 在识别、提取和分析图像及其内含文本中的细节方面有明显增强； 支持百万像素以上的高清晰度图像以及各种宽高比的图像。 Model Name 模型描述 qwen-vl-plus Qwen的增强型大规模视觉语言模型。该模型针对细节识别能力和文本识别能力进行了显著升级，支持高达数百万像素的超高清分辨率及任意图像输入的宽高比。它在各类视觉任务上都展现出卓越的性能表现。 qwen-vl-max Qwen的最强大视觉语言模型。相较于增强版本，该模型在视觉推理和指令执行能力上做出了进一步提升，提供了更高级别的视觉感知与认知理解力,在更广泛复杂的任务上都能实现最优性能。 相比于开源版本的Qwen-VL，这两个模型在多个文本-图像多模态任务中与Gemini Ultra和GPT-4V的表现相当，显著超越了之前开源模型的最佳结果。值得一提的是，Qwen-VL-Max在中文问题回答和中文文本理解任务上超越了OpenAI的GPT-4V以及谷歌的Gemini。下文展示了实验结果及真实用例。
Model DocVQA
Document understanding ChartQA
Chart understanding AI2D
Science diagrams TextVQA
Text reading MMMU
College-level problems MathVista
Mathematical reasoning MM-Bench-CN
Natural image QA in Chinese Other Best
Open-source LVLM 81.6%
(CogAgent) 68.4%
(CogAgent) 73.7%
(Fuyu-Medium) 76.1%
(CogAgent) 45.9%
(Yi-VL-34B) 36.7%
(SPHINX-V2) 72.4%
(InternLM-XComposer-VL) Gemini Pro 88.1% 74.1% 73.9% 74.6% 47.9% 45.2% 74.3% Gemini Ultra 90.9% 80.8% 1 79.</description>
    </item>
    
    <item>
      <title>Qwen介绍</title>
      <link>http://qwenlm.github.io/zh/blog/qwen/</link>
      <pubDate>Tue, 23 Jan 2024 22:13:29 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/zh/blog/qwen/</guid>
      <description>四个月前，我们首次发布Qwen-7B大型语言模型（LLM），正式开启了我们的开源之旅。今天，我们介绍Qwen开源家族，更全面的展示我们的工作和目标。下面是开源项目和社区的重要链接。
PAPER GITHUB HUGGING FACE MODELSCOPE DISCORD
Additionally, we have WeChat groups for chatting and we invite you to join the groups through the provided link in our GitHub readme.
总览 整体上，Qwen不仅仅是一个语言模型，而是一个致力于实现通用人工智能（AGI）的项目，目前包含了大型语言模型（LLM）和大型多模态模型（LMM）。下图展示了Qwen的主要组成部分:
在这里，“Qwen” 指的是基础语言模型，而 “Qwen-Chat” 则指的是通过后训练技术如SFT（有监督微调）和RLHF（强化学习人类反馈）训练的聊天模型。我们还有提供了专门针对特定领域和任务的模型，例如用于编程的 “Code-Qwen” 和用于数学的 “Math-Qwen”。大型语言模型（LLM）可以通过模态对齐扩展到多模态，因此我们有视觉-语言模型 “Qwen-VL” 以及音频-语言模型 “Qwen-Audio” 。值得注意的是，本篇博客仅介绍语言模型，至于多模态模型（LMM），例如Qwen-VL和Qwen-Audio，请参阅其各自的博客。
基础模型：对齐的良好起点 构建助手模型的一般流程包括预训练和后训练，后者主要由SFT（有监督微调）和RLHF（强化学习人类反馈）组成。至于预训练，与之前的大语言模型GPT-3、Llama类似，Qwen是一个基于Transformer的语言模型，通过预测下一个词的任务进行预训练。为了简化和稳定性，我们没有为语言模型引入更多的任务，而是专注于模型规模的扩展和数据的扩展。目前，我们已经开发了5种不同大小的模型，其中4种已开源，包括 1.8B、Qwen-7B、Qwen-14B和Qwen-72B。
Model Release Date Max Length System Prompt Enhancement # of Pretrained Tokens Minimum GPU Memory Usage of Finetuning (Q-Lora) Minimum GPU Usage of Generating 2048 Tokens (Int4) Tool Usage Qwen-1.</description>
    </item>
    
    <item>
      <title>OFA：走向通用统一模型</title>
      <link>http://qwenlm.github.io/zh/blog/ofa/</link>
      <pubDate>Mon, 14 Nov 2022 16:01:41 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/zh/blog/ofa/</guid>
      <description>2022年可以说是属于通用模型的一年！随着多模态预训练的蓬勃发展，尤其是通用模型，我们看到实现一个具有处理多种模态的多种任务的能力的通用模型的机会。因此我们提出OFA1，即One-For-All。它是一个统一的多模态预训练模型，以统一的模型架构和任务形式兼容多模态和单模态的理解与生成任务。我们使用多模态多任务的方式预训练OFA，使其成为一个接近全能的模型。我们将OFA的模型和代码全部开源到社区，希望能推动通用模型的发展。
论文 GitHub ModelScope 体验
背景 自BERT2成功迁移到多模态领域，多模态预训练蓬勃发展，代表性的工作包括UNITER3、VilBERT4等。这些工作将基于Transformer的BERT2结合到单流或双流的架构中，并将图像处理成物体特征接入Transformer中。而到了2021年，随着ViT5的兴起，越来越多放弃物体特征的工作出现，不再依赖复杂的如Faster-RCNN6的流程，比如最简单的使用patch映射ViLT7、使用CLIP8的CLIP-ViL9，等等。而SimVLM10作为这个领域一个代表工作，利用了T5/BART的特性将理解和生成任务兼容并实现多项最优表现。这些进展都奠定了通用统一模型的发展基础，2022年涌现了一批工作，包括我们的OFA、Unified-IO11、Flamingo12、BeiT-313等。
方法 OFA希望实现的是任务、模态和架构的统一。我们提出统一模型应当具备三大特性，即任务无关、模态无关和任务全面性。任务无关即统一模型应当能接受多种任务形式而无需针对性做模型结构和训练方法的改变，模态无关即不需针对模态做特定模型结构和训练方法修改，任务全面性即模型应当尽可能学习多的任务从而让自己能力更全面更能融会贯通迁移到没学习过的新任务上。因此我们提出了三大统一，即模态、架构和任务的统一。下面我们逐一解释。
统一模态最大的难题是不同模态的离散化表示，不然我们就需要使用diffusion模型14来解决问题。文本表示无需改动，主要的变化在于图像和物体框。得益于近年来vector quantization的快速发展1516以及基于Transformer的文本生成图像模型1718，图像可以用VQ token来进行表示。而针对物体框，则可以采用分桶的方式对连续的坐标值实现离散化。
模型架构上我们选择的是基于Transformer的编码-解码器。它已经在NLP领域取得巨大成功，如T519。而对于图像输入，我们使用ResNet的前3个stage。而对于Transformer，我们加入Normformer20的方法提升模型训练稳定性和最终迁移效果。
多任务学习是OFA的一大特点。我们使用了8个任务做预训练，其中包括5个图文任务、2个视觉任务和1个自然语言任务。图文任务包括视觉定位、定位物体描述、视觉问答、图文匹配和图像描述，视觉任务包括目标检测和图像还原，语言任务则是文本还原。为了让模型识别不同任务，我们为每个任务增加相应的文本提示，说明任务的内容。我们希望模型遇到新的提示能实现零样本学习。
我们使用了公开数据集进行预训练。我们希望研究人员能够利用我们的开源复现相应的结果。
我们一共开源了5个规模的模型，分别是OFA-Tiny (33M)，OFA-Medium (93M)，OFA-Base (180M)，OFA-Large(470M)和OFA-Huge (930M)。具体数据查看下表。
实验 我们在多模态和单模态任务上都做了实验。在视觉-语言理解上，我们在视觉问答和视觉推理上做了实验。在VQA上，OFA的效果和800亿参数的Flamingo以及基于50亿图文数据训练的20亿参数的CoCa效果相当，并且在视觉推理上取得最优成绩。而在视觉-语言生成上，OFA在两个阶段的图像描述评测均取得最优效果。而在视觉定位任务上，base规模的模型就可以超出此前最好的模型，也反映了生成式的方法和多任务训练的有效性，并且随着规模的增加，模型效果也能实现稳定增长。
此外，我们还在文本图像生成任务上做了测试，因为预训练中我们设计了图像还原任务，模型应当具备一定的图像生成能力。可以看到OFA可以达到非常低的FID分数，并且在更大的数据上微调能明显提升它的生成效果。
单模态方面，我们在GLUE上验证OFA的自然语言理解能力，Gigaword上验证自然语言生成能力，以及ImageNet上验证视觉理解能力。可以看到OFA在GLUE上可以取得匹敌RoBERTa和DeBERTa的效果，而此前的多模态模型距离最优的自然语言模型差距都很大。在摘要生成上，OFA也取得最优效果。而视觉分类上，OFA也能取得匹敌BeiT21和MAE22的效果。
另外，我们观察到OFA具备一定的新任务和新领域的迁移能力。如下图所示。
这个例子说明OFA对提示的理解和组合多项技能的能力。我们设计了一个基于定位的视觉问答的任务，相当于视觉问答和定位描述的组合。具体实现方式就是修改任务的提示。包含了问题和定位信息的提示引导模型做出了正确的回答。
另外，OFA迁移到新领域的能力也比较不错。在动画领域数据的定位上，模型能实现较为精准的物体定位。这是因为模型学过这个任务而预训练又见过该领域数据。这也体现了模型的组合能力。
总结 这是我们通用统一模型研究工作的起点。我们认为这个方向很有前景，因为Transformer有着一统天下的趋势，同时它能很好地兼容多种模态和任务。我们相信，多模态领域很快也会迎来属于它的GPT-323！
Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., &amp;amp; Yang, H. (2022). Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework. International Conference on Machine Learning.&amp;#160;&amp;#x21a9;&amp;#xfe0e;
Devlin, J., Chang, M.</description>
    </item>
    
    <item>
      <title>OFASys：一行代码带你搞定多任务学习！</title>
      <link>http://qwenlm.github.io/zh/blog/ofasys/</link>
      <pubDate>Wed, 28 Dec 2022 18:01:21 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/zh/blog/ofasys/</guid>
      <description>引言 通用模型非常火！我们现在跟随多模态多任务学习的发展似乎看到了实现一个真正的通用模型的机会。我们此前推出的OFA便是朝着这个目标迈向的重要一步。但是，我们在实际实现过程中遇到了非常多的困难。比如说，把多任务训练的模型搭建起来，组织多任务的训练比如给数据打batch和保证训练稳定等等，都非常困难。因此，我们推出一个AI系统OFASys，它主要解决多模态多任务学习的实现问题。简单来说，它主要通过一个叫做“Instruction”的接口来实现。Instruction 即指定了任务描述和输入信息的模板。因此，用户只需要写一行代码构建好自己的 Instruction，就可以构建一个多模态多任务学习的任务。后续的复杂步骤用户无需关心，包括数据处理、模型构建和训练等。总而言之，OFASys帮你摆脱很多复杂的实现细节，让你专注于设计任务和模态组合等。
论文 GitHub
背景 Transformer和预训练技术的蓬勃发展让我们看到了实现通用AI的机会。在自然语言领域，我们见证了GPT-31以及不可思议的ChatGPT及其背后的GPT-3.5系列，它们都具备了极强的问答、对话、以及文字创作能力等。而在多模态领域，我们也看到通用统一模型的快速崛起，包括我们的OFA2、GATO3、Unified-IO4等。实现这样的模型和系统是困难的，难点往往在于实现中。算法工程师最难受的事情往往是如何实现这样一个模型，并且成功把它训起来，接入不同类型的数据和任务并且对它们实现良好的管理。尽管我们针对深度学习有了PyTorch和TensorFlow这样的基建，以及很多诸如Hugging Face Transformers和fairseq这样的帮助实现Transformer的框架，但当前依然没有一个专门针对多模态多任务学习的提供良好抽象和相关工具的系统。
用户接口 介绍系统设计前，我们先看看如何在OFASys中使用1行代码实现一个多任务学习模型。具体而言，你需要写一个合适的 Instruction 。下面是一些例子。
以上例子中，两个句子用“-&amp;gt;”分隔，表示输入和期望输出。“&amp;lt;tt&amp;gt;[IMAGE:img]&amp;lt;/tt&amp;gt;”表示有一个图像输入和 数据集中的&amp;lt;tt&amp;gt;img&amp;lt;/tt&amp;gt;字段关联。Instruction 中的文本则表示这个任务是图像描述。任务输出是文本序列，即对应数据集中的&amp;lt;tt&amp;gt;cap&amp;lt;/tt&amp;gt;字段.
另一个例子是自然语言推理的例子，我们以MNLI为例：
和上述例子类似，我们用模板和指示词来构建 Instruction 。不同的是，输入侧包含2个输入。此外，由于我们发现在解码器重复输入有助于效果提升，我们在输出端用一个&amp;lt;tt&amp;gt;no_loss&amp;lt;/tt&amp;gt;的信号表示不计算这段序列对应的损失函数。而由于标签是一个封闭集，我们用&amp;lt;tt&amp;gt;closed_set&amp;lt;/tt&amp;gt;来表示。
简而言之，使用OFASys可以让一切变得简单。你也许只需要1行代码就可以解决问题！
系统实现 一个良好的系统实现是一个易用接口的基础。系统实现的整体架构如下所示。
OFASys通过解析 Instruction 来将任务定义和任务数据传入任务计划中。每个计划有一个模型的层次结构，其中包括模态特定的前处理和后处理模块以及模态无关的计算引擎。通用模型在这里负责融合多模态输入以及获得输出。由于输入输出都是表示序列，因此通用模型是非常灵活的。通用模型的输出最终传入后处理模块得到最终输出。而诸如损失函数计算以及生成器等都有大量实现方式。
而在多任务学习场景中，有多个上述计划。OFASys共享大部分可训练的参数，这样通用模型能使用尽可能多的数据进行训练。任务调度器在这里负责安排任务优先级以及管理多任务优化，而逻辑调度器则负责安排到多台物理机上。
应用实例: OFA+ 我们训练了一个基于OFA的通用模型OFA+，它首次实现同时处理文本、图像、语音、视频和动作多种模态。具体包括一个通用的OFA+ (Generalist)以及一个基于多模态MoE的升级版本OFA+ (Generalist MoE)。对比对象则为我们此前的OFA，它需要针对每个任务单独微调，我们称之为OFA+ (Specialist)。
从上述结果可以看出，OFA+整体能保留接近95%的OFA+ (Specialist)在各个下游任务的效果，其中涵盖7种模态的23个任务。这也能看出来多任务学习不仅赋予模型实现多任务的基础能力，同时能让它在各项任务上都能达到顶级的表现。
总结 随着通用模型快速发展，特定系统和库的空缺成了一个棘手的问题。OFASys就是为了解决实现多模态多任务学习难的问题而生。我们希望它能推动多模态多任务学习的研究同时帮助实现更加通用的通用模型。
Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T.J., Child, R., Ramesh, A.</description>
    </item>
    
    <item>
      <title>Chinese CLIP: 中文图文对比学习预训练</title>
      <link>http://qwenlm.github.io/zh/blog/chinese-clip/</link>
      <pubDate>Sat, 24 Dec 2022 14:54:19 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/zh/blog/chinese-clip/</guid>
      <description>CLIP1是多模态表示学习领域一个现象级的模型。它不仅扮演基础模型，并且建立了视觉和语言的桥梁。它还推动了很多其他领域技术的发展，尤其是文本生成图像。然而，我们还需要特定语言的CLIP，尤其在现实应用中，比如跨模态检索。在此之前还没有效果较好的开源中文CLIP。因此我们希望通过这个项目推动中文多模态的发展。
论文 Github ModelScope 体验
背景 在诸如跨模态检索的图文应用中，语言往往扮演重要的角色。假设直接使用CLIP和翻译文本，翻译质量将严重影响下游任务表现。此外，预训练数据的分布也同样重要。我们希望能够有一个模型较好地建模中文世界的数据，那么CLIP的图像学习同样需要适应到中文世界图像的领域中，这些图像在很多维度都有自己的特色，比如它所蕴含的文化价值、展现的社会风貌等。
下面是一个多语言CLIP2检索的例子。不难看出，模型难以理解一些中文概念，甚至只能召回一些西方文化类似的概念的图片。
并且，我们还做了相关实验，对比原始CLIP配合翻译文本和中文CLIP在跨模态检索任务上的表现。原始CLIP的效果远低于中文CLIP，这也一定程度反映语言特定的CLIP模型的必要性。
方法 我们尽可能采用和原始CLIP一致的设定，不去增加模块或者设计复杂化的训练方法。而为了更加高效的训练，包括训练效率和最终迁移效果的提升，我们没有选择从头开始预训练，而是提出一个两阶段预训练的方法。
在第一阶段中，我们将CLIP的双塔用已有的预训练模型进行初始化，分别是CLIP的视觉侧（如ViT-B、ResNet等）以及中文RoBERTa RoBERTA-wwm-Chinese。我们冻结图像塔，通过对比学习让文本塔的输出表示和图像塔的一致。在第二阶段中，我们解冻图像塔，进行对比学习继续训练，从而图像塔也能学习建模中文领域的图像数据分布。
在可复现性方面，我们尽可能采用公开数据集，其中包括LAION-5B3的中文部分、悟空数据集4、来自VG和MSCOCO等的翻译数据。总数据量大约2亿。
我们推出了5个规模的中文CLIP模型，其中包括ResNet-50、ViT-B/16、ViT-L/14、ViT-L/14@336px和ViT-H/14。具体数据详见下表。
实验 我们做了3个跨模态检索的实验，其中包括中文原生的MUGE和英文原生的Flickr30K-CN和COCO-CN。在所有数据集上，中文CLIP都取得了最好的效果，而尤其在MUGE上中文CLIP相比此前模型的优势最为巨大。这也反映中文CLIP在中文原生数据集上能够取得更好的表现。
我们也尝试了中文CLIP的零样本分类能力，并在ELEVATER5上做了测试，具体实现包括翻译标签和提示词。实验结果也反映中文CLIP在英文原生的基准上同样能取得有竞争力的表现。
我们补充了实验说明两阶段训练方法的有效性。对比从头训练，不管是收敛效率还是最终迁移效果上，两阶段的方法都取得更好的效果，并且对比单纯的一阶段联合训练，第二阶段预训练的加入还能进一步提升效果。这也意味着当我们打造一个语言特定的中文CLIP其实不需要从头来，可以在已有模型的基础上站得更高。
Ablation studies. 局限性及未来工作 尽管上文介绍了中文CLIP的强大之处，但我们仍需要认识到当前中文CLIP还没有充分验证其作为视觉基础模型的作用。经验上，它应当在中文原生数据上表现更好。因此，在下一阶段的工作中，我们将研究构造一个针对中文多模态表示学习和视觉表示学习的基准。
欢迎大家访问我们的GitHub存储库和ModelScope模型库，并使用我们的代码和模型。希望能够帮助到你们的研究和应用！
Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., &amp;amp; Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. International Conference on Machine Learning.&amp;#160;&amp;#x21a9;&amp;#xfe0e;
Carlsson, F., Eisen, P., Rekathati, F., &amp;amp; Sahlgren, M.</description>
    </item>
    
  </channel>
</rss>

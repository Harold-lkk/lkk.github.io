<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Blog on Qwen</title>
    <link>http://qwenlm.github.io/blog/</link>
    <description>Recent content in Blog on Qwen</description>
    <image>
      <url>http://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>http://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Fri, 07 Jun 2024 00:00:00 +0800</lastBuildDate><atom:link href="http://qwenlm.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Hello Qwen2</title>
      <link>http://qwenlm.github.io/blog/qwen2/</link>
      <pubDate>Fri, 07 Jun 2024 00:00:00 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/blog/qwen2/</guid>
      <description>GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
Introduction After months of efforts, we are pleased to announce the evolution from Qwen1.5 to Qwen2. This time, we bring to you:
Pretrained and instruction-tuned models of 5 sizes, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, and Qwen2-72B; Having been trained on data in 27 additional languages besides English and Chinese; State-of-the-art performance in a large number of benchmark evaluations; Significantly improved performance in coding and mathematics; Extended context length support up to 128K tokens with Qwen2-7B-Instruct and Qwen2-72B-Instruct.</description>
    </item>
    
    <item>
      <title>Generalizing an LLM from 8k to 1M Context using Qwen-Agent</title>
      <link>http://qwenlm.github.io/blog/qwen-agent-2405/</link>
      <pubDate>Thu, 06 Jun 2024 11:59:59 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/blog/qwen-agent-2405/</guid>
      <description>We&amp;rsquo;ve created an agent using Qwen2 models with an 8k context size to understand documents with 1M tokens, surpassing RAG and native long-context models. This agent was also used to generate data for training new long-context Qwen models.</description>
    </item>
    
    <item>
      <title>Notes on Qwen-Max-0428</title>
      <link>http://qwenlm.github.io/blog/qwen-max-0428/</link>
      <pubDate>Sat, 11 May 2024 18:10:00 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/blog/qwen-max-0428/</guid>
      <description>API DEMO DISCORD
Previously, we opensourced a series of Qwen1.5 model ranging from 0.5 to 110 billion parameters. Now, we release a larger model, Qwen-Max-0428. Qwen-Max-0428 is an instruction-tuned model for chat service. Very recently, it is available via Chatbot Arena and it has now become the top-10 in the leaderboard. Furthermore, our evaluation of MT-Bench also demonstrates that the new model outperforms our previous largest model Qwen1.5-110B-Chat.
Models MT-Bench Arena Qwen1.</description>
    </item>
    
    <item>
      <title>Qwen1.5-110B: The First 100B&#43; Model of the Qwen1.5 Series</title>
      <link>http://qwenlm.github.io/blog/qwen1.5-110b/</link>
      <pubDate>Thu, 25 Apr 2024 13:33:00 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/blog/qwen1.5-110b/</guid>
      <description>GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
Introduction Recently we have witnessed a burst of large-scale models with over 100 billion parameters in the opensource community. These models have demonstrated remarkable performance in both benchmark evaluation and chatbot arena. Today, we release the first 100B+ model of the Qwen1.5 series, Qwen1.5-110B, which achieves comparable performance with Meta-Llama3-70B in the base model evaluation, and outstanding performance in the chat evaluation, including MT-Bench and AlpacaEval 2.</description>
    </item>
    
    <item>
      <title>Code with CodeQwen1.5</title>
      <link>http://qwenlm.github.io/blog/codeqwen1.5/</link>
      <pubDate>Tue, 16 Apr 2024 13:33:00 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/blog/codeqwen1.5/</guid>
      <description>GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
Introduction The advent of advanced programming tools, which harnesses the power of large language models (LLMs), has significantly enhanced programmer productivity and accuracy. Notwithstanding these advancements, dominant coding assistants like Github Copilot, built upon proprietary LLMs, pose notable challenges in terms of cost, privacy, security, and potential copyright infringement. Recognizing the imperative for a more transparent and accessible alternative, the open-source community has embarked on a concerted endeavor to develop open codeLLMs.</description>
    </item>
    
    <item>
      <title>Qwen1.5-32B: Fitting the Capstone of the Qwen1.5 Language Model Series</title>
      <link>http://qwenlm.github.io/blog/qwen1.5-32b/</link>
      <pubDate>Tue, 02 Apr 2024 13:33:00 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/blog/qwen1.5-32b/</guid>
      <description>GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
Introduction The open-source community has long sought a model that strikes an ideal balance between performance, efficiency, and memory footprint. Despite the emergence of cutting-edge models like Qwen1.5-72B and DBRX, the models have faced persistent challenges such as large memory consumption, slow inference speed, and substantial finetuning costs.
A growing consensus within the field now points to a model with approximately 30 billion parameters as the optimal &amp;ldquo;sweet spot&amp;rdquo; for achieving both strong performance and manageable resource requirements.</description>
    </item>
    
    <item>
      <title>Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters</title>
      <link>http://qwenlm.github.io/blog/qwen-moe/</link>
      <pubDate>Thu, 28 Mar 2024 11:31:44 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/blog/qwen-moe/</guid>
      <description>GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
Introduction Since the surge in interest sparked by Mixtral, research on mixture-of-expert (MoE) models has gained significant momentum. Both researchers and practitioners are keenly interested in understanding how to effectively train such models and assessing their efficiency and effectiveness. Today, we introduce Qwen1.5-MoE-A2.7B, a small MoE model with only 2.7 billion activated parameters yet matching the performance of state-of-the-art 7B models like Mistral 7B and Qwen1.</description>
    </item>
    
    <item>
      <title>Introducing Qwen1.5</title>
      <link>http://qwenlm.github.io/blog/qwen1.5/</link>
      <pubDate>Sun, 04 Feb 2024 13:33:00 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/blog/qwen1.5/</guid>
      <description>GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
Introduction In recent months, our focus has been on developing a &amp;ldquo;good&amp;rdquo; model while optimizing the developer experience. As we progress towards Qwen1.5, the next iteration in our Qwen series, this update arrives just before the Chinese New Year.
With Qwen1.5, we are open-sourcing base and chat models across six sizes: 0.5B, 1.8B, 4B, 7B, 14B, 32B, 72B, and 110B, and also an MoE model (see blog for more information).</description>
    </item>
    
    <item>
      <title>Introducing Qwen-VL</title>
      <link>http://qwenlm.github.io/blog/qwen-vl/</link>
      <pubDate>Thu, 25 Jan 2024 13:33:00 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/blog/qwen-vl/</guid>
      <description>Along with the rapid development of our large language model Qwen, we leveraged Qwenâ€™s capabilities and unified multimodal pretraining to address the limitations of multimodal models in generalization, and we opensourced multimodal model Qwen-VL in Sep. 2023. Recently, the Qwen-VL series has undergone a significant upgrade with the launch of two enhanced versions, Qwen-VL-Plus and Qwen-VL-Max. The key technical advancements in these versions include:
Substantially boost in image-related reasoning capabilities; Considerable enhancement in recognizing, extracting, and analyzing details within images and texts contained therein; Support for high-definition images with resolutions above one million pixels and images of various aspect ratios.</description>
    </item>
    
    <item>
      <title>Introducing Qwen</title>
      <link>http://qwenlm.github.io/blog/qwen/</link>
      <pubDate>Tue, 23 Jan 2024 22:13:29 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/blog/qwen/</guid>
      <description>4 months after our first release of Qwen-7B, which is the starting point of our opensource journey of large language models (LLM), we now provide an introduction to the Qwen series to give you a whole picture of our work as well as our objectives. Below are important links to our opensource projects and community.
PAPER GITHUB HUGGING FACE MODELSCOPE DISCORD
Additionally, we have WeChat groups for chatting and we invite you to join the groups through the provided link in our GitHub readme.</description>
    </item>
    
    <item>
      <title>OFA: Towards Building a One-For-All Model</title>
      <link>http://qwenlm.github.io/blog/ofa/</link>
      <pubDate>Mon, 14 Nov 2022 16:01:41 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/blog/ofa/</guid>
      <description>2022 is a year of generalist models! With the bloom of multimodal pretraining, especially the unified model, we have witnessed the opportunity to building a generalist model that is capable of processing tasks of different modalities or multi-modalities! Thus, we propose OFA1, namely One-For-All, a unified multimodal pretrained model that unifies understanding and generation tasks concerning modalities into a single framework, and we pretrain OFA with the instruction-based multitask-pretraining that endows it with multiple capabilities.</description>
    </item>
    
    <item>
      <title>OFASys: Enabling Multitask Learning with One Line of Code! </title>
      <link>http://qwenlm.github.io/blog/ofasys/</link>
      <pubDate>Wed, 28 Dec 2022 18:01:21 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/blog/ofasys/</guid>
      <description>Intro Generalist Models are hot! We all see an opportunity towards a real generalist model by multimodal multitask learning. We previously release an opensourced unified multimodal pretrained model OFA for this goal. However, we actually met a lot of difficulties in our implementation. For example, it is hard to set up multiple tasks concerning multiple modalities, and it is hard to organize multitask learning, e.g., how to batchify your data and how to make your training stable.</description>
    </item>
    
    <item>
      <title>Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese</title>
      <link>http://qwenlm.github.io/blog/chinese-clip/</link>
      <pubDate>Sat, 24 Dec 2022 14:54:19 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/blog/chinese-clip/</guid>
      <description>CLIP1 is a phenomenal playmaker in vision and multimodal representation learning. It plays not only as a foundation model but also a bridge between vision and language. It has triggered a series of research in different fields, especially text-to-image generation. However, we find that there is a necessity for a language-specific CLIP for applications, especially cross-modal retrieval, and there is no opensourced Chinese CLIP with good performance. We therefore launched this project to promote the Chinese multimodal representation learning.</description>
    </item>
    
  </channel>
</rss>
